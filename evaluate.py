# T√™n file: evaluate.py
import streamlit as st
import os
import torch
from PIL import Image
import clip
import pandas as pd
from pathlib import Path

# --- C·∫§U H√åNH (Gi·ªëng h·ªát file app.py) ---
IMAGE_DIR = "D:/DoAn/images" 
MODEL_PATH = "checkpoints/clip_best.pt"

# --- T·∫¢I MODEL V√Ä D·ªÆ LI·ªÜU (T∆∞∆°ng t·ª± file app.py) ---
@st.cache_resource
def load_model_and_index_images():
    # ... (Sao ch√©p y h·ªát h√†m load_model_and_index_images t·ª´ file app.py c·ªßa b·∫°n) ...
    st.info("B·∫Øt ƒë·∫ßu t·∫£i model v√† l·∫≠p ch·ªâ m·ª•c cho kho ·∫£nh...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load('ViT-B/32', device=device, jit=False)
    try:
        checkpoint = torch.load(MODEL_PATH, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
    except Exception as e:
        st.error(f"L·ªói khi ƒë·ªçc file checkpoint: {e}")
        return None, None, None, None, None
    image_paths = []
    ground_truths = {}
    for root, _, files in os.walk(IMAGE_DIR):
        for file in files:
            if file.lower().endswith((".jpg", ".jpeg", ".png")):
                path = os.path.join(root, file)
                image_paths.append(path)
                # L·∫•y t√™n th∆∞ m·ª•c cha l√†m nh√£n (ground truth)
                ground_truths[path] = Path(path).parent.name
    
    if not image_paths:
        st.error(f"Kh√¥ng t√¨m th·∫•y file ·∫£nh n√†o trong th∆∞ m·ª•c: {IMAGE_DIR}")
        return None, None, None, None, None

    all_image_features = []
    # ... (Ph·∫ßn x·ª≠ l√Ω ·∫£nh gi·ªØ nguy√™n nh∆∞ trong app.py) ...
    for path in image_paths:
        try:
            img = Image.open(path).convert("RGB")
            preprocessed_img = preprocess(img).unsqueeze(0).to(device)
            with torch.no_grad():
                feat = model.encode_image(preprocessed_img)
                feat /= feat.norm(dim=-1, keepdim=True)
            all_image_features.append(feat)
        except Exception:
            continue
    image_features_tensor = torch.cat(all_image_features, dim=0)
    st.success(f"ƒê√£ t·∫£i model v√† l·∫≠p ch·ªâ m·ª•c th√†nh c√¥ng {len(image_paths)} ·∫£nh!")
    return model, device, image_features_tensor, image_paths, ground_truths

# --- H√ÄM T√çNH TO√ÅN ƒê·ªò CH√çNH X√ÅC ---
def calculate_top_k_accuracy(model, device, image_features, image_paths, ground_truths, k_value):
    # L·∫•y danh s√°ch c√°c nh√£n duy nh·∫•t t·ª´ t√™n th∆∞ m·ª•c
    unique_labels = sorted(list(set(ground_truths.values())))
    
    hits = 0
    results_data = []

    progress_bar = st.progress(0, text=f"ƒêang ƒë√°nh gi√° {len(unique_labels)} nh√£n...")

    for i, label in enumerate(unique_labels):
        # T·∫°o c√¢u truy v·∫•n t·ª´ nh√£n
        query = f"a photo of a {label.replace('_', ' ')}"
        
        # T√¨m ki·∫øm
        with torch.no_grad():
            text_input = clip.tokenize([query]).to(device)
            text_features = model.encode_text(text_input)
            text_features /= text_features.norm(dim=-1, keepdim=True)
            similarity = (100.0 * text_features @ image_features.T).softmax(dim=-1)
            _, indices = similarity[0].topk(k_value)
        
        # L·∫•y K k·∫øt qu·∫£ h√†ng ƒë·∫ßu
        top_k_paths = [image_paths[idx] for idx in indices]
        
        # Ki·ªÉm tra xem c√≥ "hit" hay kh√¥ng
        is_hit = False
        for path in top_k_paths:
            if ground_truths.get(path) == label:
                is_hit = True
                hits += 1
                break # N·∫øu ƒë√£ hit th√¨ kh√¥ng c·∫ßn ki·ªÉm tra n·ªØa
        
        results_data.append({
            "Nh√£n (Th∆∞ m·ª•c)": label,
            "C√¢u truy v·∫•n": query,
            "D·ª± ƒëo√°n ƒë√∫ng?": "‚úÖ ƒê√∫ng" if is_hit else "‚ùå Sai",
            "Top K k·∫øt qu·∫£ tr·∫£ v·ªÅ": [Path(p).name for p in top_k_paths]
        })
        
        progress_bar.progress((i + 1) / len(unique_labels), text=f"ƒêang ƒë√°nh gi√° nh√£n: {label}")
    
    progress_bar.empty()
    accuracy = (hits / len(unique_labels)) * 100
    return accuracy, pd.DataFrame(results_data)

# --- GIAO DI·ªÜN WEB ---
st.set_page_config(page_title="ƒê√°nh gi√° m√¥ h√¨nh", layout="wide")
st.title("üìä B√°o c√°o ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh t√¨m ki·∫øm")

# T·∫£i d·ªØ li·ªáu
model, device, image_features, image_paths, ground_truths = load_model_and_index_images()

if model:
    st.header("Thi·∫øt l·∫≠p ƒë√°nh gi√°")
    k_value = st.slider(
        "Ch·ªçn gi√° tr·ªã K (Top-K Accuracy)",
        min_value=1,
        max_value=10,
        value=5, # M·∫∑c ƒë·ªãnh l√† Top-5
        help="M√¥ h√¨nh s·∫Ω ƒë∆∞·ª£c coi l√† 'ƒëo√°n ƒë√∫ng' n·∫øu k·∫øt qu·∫£ ch√≠nh x√°c n·∫±m trong Top-K ·∫£nh tr·∫£ v·ªÅ."
    )

    if st.button(f"üöÄ B·∫Øt ƒë·∫ßu t√≠nh to√°n Top-{k_value} Accuracy", type="primary"):
        with st.spinner("ƒêang th·ª±c hi·ªán ƒë√°nh gi√°, vui l√≤ng ch·ªù..."):
            accuracy, results_df = calculate_top_k_accuracy(
                model, device, image_features, image_paths, ground_truths, k_value
            )
        
        st.header("K·∫øt qu·∫£ ƒë√°nh gi√°")
        st.metric(label=f"ƒê·ªô ch√≠nh x√°c Top-{k_value}", value=f"{accuracy:.2f} %")
        
        st.info(f"Trong t·ªïng s·ªë {len(results_df)} nh√£n, m√¥ h√¨nh ƒë√£ d·ª± ƒëo√°n ƒë√∫ng {int(accuracy/100*len(results_df))} nh√£n.")
        
        st.header("Chi ti·∫øt t·ª´ng truy v·∫•n")
        st.dataframe(results_df)