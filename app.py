# T√™n file: app.py
# Sao ch√©p v√† d√°n to√†n b·ªô code n√†y v√†o file app.py c·ªßa b·∫°n.

import streamlit as st
import os
import torch
from PIL import Image
import clip
import time

# --- C·∫§U H√åNH ---
IMAGE_DIR = "D:/DoAn/images" 
MODEL_PATH = "checkpoints/clip_best.pt"

# --- LOGIC BACKEND ---

@st.cache_resource(show_spinner="ƒêang t·∫£i model v√† l·∫≠p ch·ªâ m·ª•c cho kho ·∫£nh...")
def load_model_and_index_images():
    """
    T·∫£i model CLIP v√† x·ª≠ l√Ω ·∫£nh.
    H√†m n√†y ƒë∆∞·ª£c cache v√† KH√îNG ch·ª©a b·∫•t k·ª≥ l·ªánh giao di·ªán streamlit n√†o.
    Th√¥ng b√°o t·∫£i s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi show_spinner.
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    try:
        model, preprocess = clip.load('ViT-B/32', device=device, jit=False)
        checkpoint = torch.load(MODEL_PATH, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        print("T·∫£i th√†nh c√¥ng tr·ªçng s·ªë model t·ª´ file checkpoint!")
    except Exception as e:
        raise RuntimeError(f"L·ªói khi t·∫£i model: {e}. H√£y ch·∫Øc ch·∫Øn file checkpoint t·ªìn t·∫°i v√† h·ª£p l·ªá.") from e

    image_paths = [os.path.join(root, file) for root, _, files in os.walk(IMAGE_DIR) for file in files if file.lower().endswith((".jpg", ".jpeg", ".png"))]
    
    if not image_paths:
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file ·∫£nh n√†o trong th∆∞ m·ª•c: {IMAGE_DIR}")

    all_image_features = []
    valid_paths = []
    
    for path in image_paths:
        try:
            img = Image.open(path).convert("RGB")
            preprocessed_img = preprocess(img).unsqueeze(0).to(device)
            with torch.no_grad():
                feat = model.encode_image(preprocessed_img)
                feat /= feat.norm(dim=-1, keepdim=True)
            all_image_features.append(feat)
            valid_paths.append(path)
        except Exception as e:
            print(f"B·ªè qua ·∫£nh l·ªói {path}: {e}")
            
    if not all_image_features:
        raise ValueError("Kh√¥ng th·ªÉ x·ª≠ l√Ω b·∫•t k·ª≥ ·∫£nh n√†o. Vui l√≤ng ki·ªÉm tra ƒë·ªãnh d·∫°ng ·∫£nh.")

    image_features_tensor = torch.cat(all_image_features, dim=0)
    print(f"ƒê√£ l·∫≠p ch·ªâ m·ª•c th√†nh c√¥ng {len(valid_paths)} ·∫£nh!")
    
    return model, device, image_features_tensor, valid_paths

def search_images(query, model, device, image_features, image_paths, top_k=5):
    with torch.no_grad():
        text_input = clip.tokenize([query]).to(device)
        text_features = model.encode_text(text_input)
        text_features /= text_features.norm(dim=-1, keepdim=True)
        similarity = (100.0 * text_features @ image_features.T).softmax(dim=-1)
        values, indices = similarity[0].topk(top_k)
    return [(image_paths[i], float(v)) for v, i in zip(values, indices)]

# --- GIAO DI·ªÜN WEB (FRONTEND) ---

st.set_page_config(page_title="T√¨m ki·∫øm h√¨nh ·∫£nh xe", page_icon="üöó", layout="wide")

if 'first_load_success' not in st.session_state:
    st.session_state.first_load_success = True

try:
    model, device, image_features, image_paths = load_model_and_index_images()

    if st.session_state.first_load_success:
        st.success(f"ƒê√£ l·∫≠p ch·ªâ m·ª•c th√†nh c√¥ng {len(image_paths)} ·∫£nh! H·ªá th·ªëng ƒë√£ s·∫µn s√†ng.")
        time.sleep(2)
        st.session_state.first_load_success = False
        st.rerun()

    col1, col2, col3 = st.columns([2,3,2])
    with col2:
        
        
        st.markdown(
            "<h1 style='text-align: center; white-space: nowrap;'>T√¨m Ki·∫øm H√¨nh ·∫¢nh Xe Th√¥ng Minh</h1>", 
            unsafe_allow_html=True
        )
        

    # Ph·∫ßn T√πy ch·ªçn
    with st.expander("‚öôÔ∏è T√πy ch·ªçn t√¨m ki·∫øm"):
        top_k = st.slider(
            "S·ªë l∆∞·ª£ng k·∫øt qu·∫£ hi·ªÉn th·ªã", 
            min_value=1, 
            max_value=20, 
            value=6, 
            step=1
        )

    if 'query' not in st.session_state:
        st.session_state.query = ""

    with st.form(key='search_form'):
        query_input = st.text_input(
            "M√¥ t·∫£ xe b·∫°n mu·ªën t√¨m ki·∫øm...",
            value=st.session_state.query,
            placeholder="v√≠ d·ª•: xe SUV m√†u tr·∫Øng, xe b√°n t·∫£i m√†u ƒëen...",
            label_visibility="collapsed"
        )
        submit_button = st.form_submit_button(label='üîç T√¨m ki·∫øm')

    if submit_button and query_input:
        st.session_state.query = query_input

    if st.session_state.query:
        st.write("---") 
        st.subheader(f"K·∫øt qu·∫£ t√¨m ki·∫øm cho: '{st.session_state.query}'")
        
        results = search_images(st.session_state.query, model, device, image_features, image_paths, top_k)
        
        if not results:
            st.warning("R·∫•t ti·∫øc, kh√¥ng t√¨m th·∫•y h√¨nh ·∫£nh n√†o ph√π h·ª£p v·ªõi m√¥ t·∫£ c·ªßa b·∫°n.")
        else:
            num_columns = 3 
            cols = st.columns(num_columns)
            for i, (img_path, score) in enumerate(results):
                with cols[i % num_columns]:
                    st.image(
                        img_path,
                        use_container_width=True,
                        caption=f"ƒê·ªô kh·ªõp: {score*100:.2f}%"
                    )

except (RuntimeError, FileNotFoundError, ValueError) as e:
    st.error(f"**ƒê√£ x·∫£y ra l·ªói nghi√™m tr·ªçng:**\n\n{e}\n\nVui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n file v√† c·∫•u h√¨nh, sau ƒë√≥ l√†m m·ªõi l·∫°i trang.")